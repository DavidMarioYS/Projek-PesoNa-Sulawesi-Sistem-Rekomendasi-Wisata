# -*- coding: utf-8 -*-
"""Copy of [FINAL] Projek Akhir 1 : PesoNa - Sistem Rekomendasi Wisata - Sulawesi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LBVskuMZ7t8bCpIt66tGb2ltqnjTpmGS

**SISTEM REKOMENDASI DESTINASI WISATA PESONA**

---
username : `davidmarioys`

# Data Wrangling
  1. Gathering Data = Mengidentifikasi libraries dan Membaca dataset
  2. Assessing Data = Memeriksa dan memahami data
  3. Cleaning Data = Membersihkan data dari kesalahan/error

## Gathering Data

### Import Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
import folium
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate
from sklearn.metrics import mean_squared_error, mean_absolute_error

"""### Upload Dataset"""

# Mengunggah file dari komputer lokal
uploaded = files.upload()

"""### Load & Read Dataset"""

# Load Dataset
df = pd.read_csv('./Sulawesi.csv')

print(f'Dataset PesoNa: \n{df}')

"""## Assessing Data"""

df.info()

df.describe()

print('IDENTIFIKASI DATA')
print(f'\nData Kosong: {df.isnull().sum().sum()}')
print(f'Data Duplikat: {df.duplicated().sum()}')
print(f'Jumlah Data: {df.shape[0]}')
print(f'Jumlah Kolom: {df.shape[1]}')

"""Dataset Sulawesi tidak memiliki data kosong namun memiliki sejumlah 123 data duplikat, dengan total data sebanyak 2891 pada 8 kolom."""

print(f'Kolom: {df.columns.tolist()}')
print(f'\nJumlah Nilai Unique: \n{df.nunique()}')

"""### Visualisasi Data Awal"""

print('VISUALISASI DATA\n')

# Subplot untuk Rating
plt.subplot(2, 2, 1)
sns.histplot(df['Rating'], kde=True, color='skyblue', bins=15)
plt.title('Distribusi Rating', fontsize=15)
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

# Subplot untuk Reviews
plt.subplot(2, 2, 2)
sns.histplot(df['Reviews'], kde=True, color='salmon', bins=20)
plt.title('Distribusi Reviews', fontsize=15)
plt.xlabel('Reviews', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

# Subplot untuk Latitude
plt.subplot(2, 2, 3)
sns.histplot(df['Latitude'], kde=True, color='green', bins=20)
plt.title('Distribusi Latitude', fontsize=15)
plt.xlabel('Latitude', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

# Subplot untuk Longitude
plt.subplot(2, 2, 4)
sns.histplot(df['Longitude'], kde=True, color='orange', bins=20)
plt.title('Distribusi Longitude', fontsize=15)
plt.xlabel('Longitude', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

plt.tight_layout()
plt.show()

"""### Matriks Korelasi"""

# Filter kolom numerik
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Hitung korelasi
correlation_matrix = df[numeric_columns].corr()

print("Matrix Korelasi Antar Variabel Numerik:")
print(correlation_matrix)

# Heatmap korelasi
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Heatmap Korelasi Antar Variabel')
plt.show()

"""## Cleaning Data

### Penanganan Data Kosong
"""

print('Menghapus Data Kosong')

# Drop rows with missing values
df.dropna(inplace=True)

# Check for missing values after dropping rows
print(df.isnull().sum())

print(f'\nIdentifikasi Data Kosong: {df.isnull().sum().sum()}')

"""### Penanganan Data Duplikat"""

print('Menghapus Data Duplikat')

# hapus data duplikat
df.drop_duplicates(inplace=True)
df.duplicated().sum()

print(f'\nIdentifikasi Data Duplikat: {df.duplicated().sum().sum()}')

"""## Penanganan Nilai yang Hilang"""

# Memeriksa apakah ada nilai yang hilang
print(f'Identifikasi Nilai Hilang: {df.isnull().sum().sum()}')

"""### Penanganan Nilai Outlier"""

# Convert potentially problematic columns to numeric types
df['Reviews'] = pd.to_numeric(df['Reviews'], errors='coerce')
df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')
df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')
df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')

print('Identifikasi Data Outlier')
numeric_columns = ['Reviews', 'Rating', 'Latitude', 'Longitude']
Q1 = df[numeric_columns].quantile(0.25)
Q3 = df[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_columns = ((df[numeric_columns] < lower_bound) | (df[numeric_columns] > upper_bound)).any(axis=1)
outliers = df[outlier_columns]

print(f'Jumlah Data Outlier: {outliers.shape[0]}')
print(f'\nData Outlier: \n{outliers}')

# Fungsi untuk menghapus outlier secara berulang
def remove_outliers(df, numeric_columns):
    while True:
        Q1 = df[numeric_columns].quantile(0.25)
        Q3 = df[numeric_columns].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outlier_columns = ((df[numeric_columns] < lower_bound) | (df[numeric_columns] > upper_bound)).any(axis=1)
        outliers = df[outlier_columns]

        if outliers.empty:
            break

        df = df[~outlier_columns]

    return df

# Menghapus outlier secara berulang
df_cleaned = remove_outliers(df, numeric_columns)

print(f'Jumlah Data Asli {df.shape}')
print(f'Jumlah Data Setelah Hapus Outlier {df_cleaned.shape}')

print("Data setelah menghapus outlier:")
print(df_cleaned)

"""### Matriks Korelasi Data Sesudah Diolah"""

# Filter kolom numerik
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Hitung korelasi
correlation_matrix = df_cleaned[numeric_columns].corr()

print("Matrix Korelasi Antar Variabel Numerik:")
print(correlation_matrix)

# Heatmap korelasi
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Heatmap Korelasi Antar Variabel')
plt.show()

"""Terdapat perubahan dari hasil matriks korelasi, yaitu:

| Variabel   | Korelasi (Orisinil) | Korelasi (Setelah Diolah) | Perubahan |
|------------|----------------------|---------------------------|-----------|
| Rating     | 1.000000             | 1.000000                  | -         |
| Reviews    | 0.013161             | -0.308007                 | Lebih rendah dan negatif setelah pengolahan data, menunjukkan hubungan yang lebih kuat dalam arah yang berlawanan antara Rating dan Reviews. |
| Latitude   | 0.084435             | 0.081939                  | Sedikit berubah, tetapi tetap positif, menunjukkan hubungan yang tetap antara Latitude dengan variabel lainnya meskipun ada pengolahan data. |
| Longitude  | 0.096408             | 0.088860                  | Sedikit berubah, tetapi tetap positif, menunjukkan hubungan yang tetap antara Longitude dengan variabel lainnya meskipun ada pengolahan data. |

Korelasi antara Rating dan Reviews mengalami perubahan drastis setelah pengolahan data, menjadi lebih negatif, menunjukkan bahwa transformasi atau pengolahan data telah mempengaruhi cara variabel-variabel ini berinteraksi dalam analisis korelasi. Korelasi antara Latitude dan Longitude, meskipun mengalami sedikit perubahan, tetap menunjukkan hubungan yang stabil dengan variabel lainnya setelah pengolahan data.

## Exploratory Data Analysis (EDA)

### Visualisasi Data
"""

print('VISUALISASI DATA\n')

# Subplot untuk Rating
plt.subplot(2, 2, 1)
sns.histplot(df_cleaned['Rating'], kde=True, color='skyblue', bins=15)
plt.title('Distribusi Rating', fontsize=15)
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

# Subplot untuk Reviews
plt.subplot(2, 2, 2)
sns.histplot(df_cleaned['Reviews'], kde=True, color='salmon', bins=20)
plt.title('Distribusi Reviews', fontsize=15)
plt.xlabel('Reviews', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

# Subplot untuk Latitude
plt.subplot(2, 2, 3)
sns.histplot(df_cleaned['Latitude'], kde=True, color='green', bins=20)
plt.title('Distribusi Latitude', fontsize=15)
plt.xlabel('Latitude', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

# Subplot untuk Longitude
plt.subplot(2, 2, 4)
sns.histplot(df_cleaned['Longitude'], kde=True, color='orange', bins=20)
plt.title('Distribusi Longitude', fontsize=15)
plt.xlabel('Longitude', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y')

plt.tight_layout()
plt.show()

# Create a scatter plot with Rating on the x-axis and Reviews on the y-axis
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Rating', y='Reviews', data=df_cleaned, color='skyblue')

# Add a regression line to the scatter plot
sns.regplot(x='Rating', y='Reviews', data=df_cleaned, color='salmon', line_kws={'linestyle': '--'})

# Set labels and title
plt.xlabel('Rating', fontsize=15)
plt.ylabel('Reviews', fontsize=15)
plt.title('Relationship between Rating and Reviews', fontsize=18)

# Show the plot
plt.show()

# Calculate the correlation coefficient between Rating and Reviews
correlation = df_cleaned['Rating'].corr(df_cleaned['Reviews'])

# Print the correlation coefficient
print(f'Correlation between Rating and Reviews: {correlation:.3f}')

# Create a scatter plot with Rating on the x-axis and Latitude on the y-axis
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Rating', y='Latitude', data=df_cleaned, color='skyblue')

# Add a regression line to the scatter plot
sns.regplot(x='Rating', y='Latitude', data=df_cleaned, color='salmon', line_kws={'linestyle': '--'})

# Set labels and title
plt.xlabel('Rating', fontsize=15)
plt.ylabel('Latitude', fontsize=15)
plt.title('Relationship between Rating and Latitude', fontsize=18)

# Show the plot
plt.show()

# Calculate the correlation coefficient between Rating and Latitude
correlation = df_cleaned['Rating'].corr(df_cleaned['Latitude'])

# Print the correlation coefficient
print(f'Correlation between Rating and Latitude: {correlation:.3f}')

# Create a scatter plot with Rating on the x-axis and Longitude on the y-axis
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Rating', y='Longitude', data=df_cleaned, color='skyblue')

# Add a regression line to the scatter plot
sns.regplot(x='Rating', y='Longitude', data=df_cleaned, color='salmon', line_kws={'linestyle': '--'})

# Set labels and title
plt.xlabel('Rating', fontsize=15)
plt.ylabel('Longitude', fontsize=15)
plt.title('Relationship between Rating and Longitude', fontsize=18)

# Show the plot
plt.show()

# Calculate the correlation coefficient between Rating and Longitude
correlation = df_cleaned['Rating'].corr(df_cleaned['Longitude'])

# Print the correlation coefficient
print(f'Correlation between Rating and Longitude: {correlation:.3f}')

"""### Jumlah Data Pada Fitur"""

df_cleaned['Provinsi'].value_counts()

df_cleaned['KabupatenKota'].value_counts()

df_cleaned['JenisWisata'].value_counts()

# Menghitung jumlah data pada kolom 'Rating' berdasarkan jumlah bintang
rating_counts = df_cleaned['Rating'].value_counts().sort_index()

# Menampilkan jumlah data untuk setiap jumlah bintang
print("Jumlah data berdasarkan jumlah bintang pada Rating:")
for rating, count in rating_counts.items():
    print(f"Rating {rating}: {count} data")

"""# Data Preprocessing"""

# Membuat peta sebaran geografis tempat wisata
map_sulawesi = folium.Map(location=[-8.123528019761865, 115.06461953075772], zoom_start=6)
for _, row in df_cleaned.iterrows():
    folium.Marker([row['Latitude'], row['Longitude']], popup=row['NamaWisata']).add_to(map_sulawesi)

map_sulawesi

df_cleaned.info()

"""### Scaling Fitur Numerik"""

# Pilih fitur numerik yang akan di-scaling
numerical_features = ['Rating', 'Reviews', 'Latitude', 'Longitude']

# Buat objek MinMaxScaler
scaler = MinMaxScaler()

# Lakukan scaling pada fitur numerik
df_cleaned[numerical_features] = scaler.fit_transform(df_cleaned[numerical_features])

# Tampilkan data yang sudah di-scaling
print("Data setelah scaling:")
print(df_cleaned[numerical_features].head())

"""### Feature Engineering - Encoding"""

# Buat objek LabelEncoder
label_encoder = LabelEncoder()

# Lakukan encoding pada kolom 'JenisWisata'
df_cleaned['JenisWisata_Encoded'] = label_encoder.fit_transform(df_cleaned['JenisWisata'])

# Simpan mapping dari label ke nilai numerik
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Tampilkan mapping
print("Mapping JenisWisata:")
print(label_mapping)

df_cleaned

# Split data into training and test sets
X = df_cleaned[['Rating', 'Reviews', 'Latitude', 'Longitude', 'JenisWisata_Encoded']]
y = df_cleaned['Rating']  # Assuming 'Rating' is the target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model untuk mendapatkan representasi vektor
input_layer = Input(shape=(5,))
embedding_layer = Dense(64, activation='relu')(input_layer)
embedding_layer = Dense(32, activation='relu')(embedding_layer)
output_layer = Dense(1, activation='linear')(embedding_layer)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
early_stop = EarlyStopping(monitor='val_loss', patience=10)

# Training model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[reduce_lr, early_stop])

# Mendapatkan representasi vektor dari data
feature_model = Model(inputs=model.input, outputs=model.layers[-2].output)
X_train_embedding = feature_model.predict(X_train)
X_test_embedding = feature_model.predict(X_test)

# Melihat hasil training model dengan detil dan menarik

import matplotlib.pyplot as plt

# Plot training & validation loss values
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

# Plot training & validation MAE values
plt.figure(figsize=(12, 6))
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Mean Absolute Error')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

# Evaluasi model pada data test
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
print('Test Loss:', test_loss)
print('Test MAE:', test_mae)

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung kemiripan menggunakan cosine similarity
similarity_matrix = cosine_similarity(X_train_embedding)

def get_recommendations(place_id, similarity_matrix, df, top_n=5):
    similarity_scores = list(enumerate(similarity_matrix[place_id]))
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
    top_places = similarity_scores[1:top_n+1]
    recommended_places = [df.iloc[i[0]]['NamaWisata'] for i in top_places]
    return recommended_places

# Contoh penggunaan fungsi rekomendasi
place_id = 124
recommended_places = get_recommendations(place_id, similarity_matrix, df_cleaned, top_n=5)
print(f'Rekomendasi Wisata: {recommended_places}')

from geopy.distance import geodesic

def get_hybrid_recommendations(place_id, similarity_matrix, df, top_n=5):
    # Mendapatkan rekomendasi berdasarkan kemiripan
    recommended_places = get_recommendations(place_id, similarity_matrix, df, top_n)

    # Mendapatkan lokasi tempat wisata acuan
    base_place_coords = df.iloc[place_id][['Latitude', 'Longitude']].values

    # Menghitung jarak ke tempat wisata yang direkomendasikan
    distances = []
    for place in recommended_places:
        place_coords = df[df['NamaWisata'] == place][['Latitude', 'Longitude']].values[0]
        distance = geodesic(base_place_coords, place_coords).kilometers
        distances.append((place, distance))

    # Mengurutkan tempat wisata berdasarkan jarak terdekat
    distances = sorted(distances, key=lambda x: x[1])

    # Mengembalikan daftar tempat wisata yang direkomendasikan berdasarkan jarak terdekat
    return [place for place, distance in distances]

# Contoh penggunaan fungsi hybrid rekomendasi
place_id = 124
hybrid_recommended_places = get_hybrid_recommendations(place_id, similarity_matrix, df_cleaned, top_n=5)
print("Rekomendasi Hybrid:")
for place in hybrid_recommended_places:
    print(place)

def evaluate_model(df, similarity_matrix, k=5):
    precision_scores = []
    mae_scores = []

    # Iterate up to the size of the similarity matrix to avoid index errors
    for idx in range(len(similarity_matrix)):
        actual_place = df.iloc[idx]['NamaWisata']
        recommended_places = get_hybrid_recommendations(idx, similarity_matrix, df, top_n=k)

        # Precision@K
        if actual_place in recommended_places:
            precision_scores.append(1)
        else:
            precision_scores.append(0)

        # MAE untuk jarak
        actual_coords = df.iloc[idx][['Latitude', 'Longitude']].values.astype(np.float32) # Cast to float32
        recommended_coords = df[df['NamaWisata'].isin(recommended_places)][['Latitude', 'Longitude']].values.astype(np.float32) # Cast to float32
        distances = np.linalg.norm(recommended_coords - actual_coords, axis=1)
        mae = np.mean(distances)
        mae_scores.append(mae)

    precision_at_k = np.mean(precision_scores)
    mean_absolute_error_distance = np.mean(mae_scores)

    return precision_at_k, mean_absolute_error_distance

precision_at_k, mean_absolute_error_distance = evaluate_model(df_cleaned, similarity_matrix, k=5)
print(f"Precision@K: {precision_at_k}")
print(f"Mean Absolute Error (MAE) untuk jarak: {mean_absolute_error_distance}")